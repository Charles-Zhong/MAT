{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import torch\n",
    "import preprocess\n",
    "import perturbation\n",
    "from torch.nn import MSELoss\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置任务和模型\n",
    "task_name = \"SST-2\" # [\"CoLA\", \"SST-2\", \"MRPC\", \"STS-B\", \"QQP\", \"MNLI-m\", \"MNLI-mm\", \"QNLI\", \"RTE\", \"WNLI\"]\n",
    "model_name = \"models/bert-base-uncased\" # [bert-base-uncased, roberta-base]\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置超参数\n",
    "# 训练参数\n",
    "seed = 42\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "# 对抗训练参数\n",
    "adv_init_epsilon = 1e-2 # 初始化扰动\n",
    "adv_init_type = \"zero\" # [\"zero\",\"rand\",\"randn\"]\n",
    "sampling_times_theta = 5 # theta采样次数\n",
    "sampling_times_delta = 3 # delta采样次数\n",
    "sampling_noise_theta = 0 # 采样噪声\n",
    "sampling_noise_delta = 0 # 采样噪声\n",
    "sampling_step_theta = 3e-5 # theta采样步长\n",
    "sampling_step_delta = 1e-2 # theta采样步长\n",
    "lambda_s = 1 # lambda λ\n",
    "beta = 0.1 # beta β"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/ailab/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- load the dataset ----------------\n",
      "[Notice]: loading dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f5f05a713dd4cc996c1b66797d6fe3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Notice]: dataset sst2 is loaded.\n",
      "--------------------------------------------------\n",
      "-------- load the tokenizer and the model --------\n",
      "[Notice]: loading tokenizer and model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at models/bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at models/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Loading cached processed dataset at /home/ailab/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-05a66768d2a80c25.arrow\n",
      "Loading cached processed dataset at /home/ailab/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-5ab7c047d4fd4ff8.arrow\n",
      "Loading cached processed dataset at /home/ailab/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-303f9218f401490d.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Notice]: tokenizer and model are loaded.\n",
      "--------------------------------------------------\n",
      "-------------- tokenize the dataset --------------\n",
      "[Notice]: tokenizing the dataset...\n",
      "[Notice]: the dataset is tokenized.\n",
      "--------------------------------------------------\n",
      "-------------- make the dataloader ---------------\n",
      "[Notice]: making dataloader...\n",
      "[Notice]: the dataloader is made.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model, dataloader, metric= preprocess.preprocess(task_name, model_name, batch_size, seed)\n",
    "train_dataloader,eval_dataloader,test_dataloader = dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ls(P, Q):\n",
    "    task_type = \"classification\" if task_name != \"STS-B\" else \"regression\"\n",
    "    if(task_type == \"classification\"):\n",
    "        return F.kl_div(P.softmax(dim=-1).log(), Q.softmax(dim=-1), reduction='batchmean') + F.kl_div(Q.softmax(dim=-1).log(), P.softmax(dim=-1), reduction='batchmean')\n",
    "    elif(task_type == \"regression\"):\n",
    "        return MSELoss(P, Q, reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGLD(z, grad, step, epsilon):\n",
    "    noise = perturbation.init_delta(z.size(), epsilon=epsilon, init_type=\"randn\")\n",
    "    z = z - step * grad + math.sqrt(2 * step) * noise\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start = time.time()\n",
    "file = None # 设置日志文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May 17 14:29:51 2022\n",
      "******************** Training ********************\n",
      "TASK: SST-2\n",
      "MODEL: models/bert-base-uncased\n",
      "DEVICE: cuda:0\n",
      "================ General Training ================\n",
      "EPOCH_NUM: 2\n",
      "BATCH_SIZE: 32\n",
      "================== MAT Training ==================\n",
      "Adversarial_Training_type: MAT\n",
      "Adversarial_init_epsilon: 0.01\n",
      "Adversarial_init_type: zero\n",
      "Sampling_times_theta: 5\n",
      "Sampling_times_delta: 3\n",
      "Sampling_noise_theta: 0\n",
      "Sampling_noise_delta: 0\n",
      "Sampling_step_theta: 3e-05\n",
      "Sampling_step_delta: 0.01\n",
      "lambda: 1\n",
      "beta: 0.1\n",
      "**************************************************\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "408d64c5eca54046a659924fdde49c00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4210 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- EPOCH: 0 --------------------\n",
      "Training...139747238954384\n",
      "139747238954384\n",
      "139747238954384\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ailab/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3465: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "print(time.ctime(), file=file)\n",
    "print(\"*\"*20, \"Training\", \"*\"*20, file=file)  # 训练任务\n",
    "print(\"TASK:\", task_name, file=file)\n",
    "print(\"MODEL:\", model_name, file=file)\n",
    "print(\"DEVICE:\", device, file=file)\n",
    "print(\"=\"*16, \"General Training\", \"=\"*16, file=file)  # 常规训练参数\n",
    "print(\"EPOCH_NUM:\", epochs, file=file)\n",
    "print(\"BATCH_SIZE:\", batch_size, file=file)\n",
    "print(\"=\"*18, \"MAT Training\", \"=\"*18, file=file)  # MAT训练参数\n",
    "print(\"Adversarial_Training_type:\", \"MAT\", file=file)\n",
    "print(\"Adversarial_init_epsilon:\", adv_init_epsilon, file=file)\n",
    "print(\"Adversarial_init_type:\", adv_init_type, file=file)\n",
    "print(\"Sampling_times_theta:\", sampling_times_theta, file=file)\n",
    "print(\"Sampling_times_delta:\", sampling_times_delta, file=file)\n",
    "print(\"Sampling_noise_theta:\", sampling_noise_theta, file=file)\n",
    "print(\"Sampling_noise_delta:\", sampling_noise_delta, file=file)\n",
    "print(\"Sampling_step_theta:\", sampling_step_theta, file=file)\n",
    "print(\"Sampling_step_delta:\", sampling_step_delta, file=file)\n",
    "print(\"lambda:\", lambda_s, file=file)\n",
    "print(\"beta:\", beta, file=file)\n",
    "print(\"*\"*50, file=file)\n",
    "model.to(device)\n",
    "progress_bar = tqdm(range(epochs * len(train_dataloader)))\n",
    "progress_bar.set_description(\"Training...\")\n",
    "eval_metric_list = []\n",
    "for i in range(epochs):\n",
    "    print(\"-\"*20, \"EPOCH:\", i, \"-\"*20, file=file)\n",
    "    print(\"Training...\", end='', file=file)\n",
    "    model.train()\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        batch = {key: batch[key].to(device) for key in batch}\n",
    "\n",
    "        # [begin] MAT Training\n",
    "        # 1.init delta & inputs\n",
    "        ## 1.1 获得batch的word_embedding\n",
    "        if \"bert-\" in model_name:\n",
    "            word_embedding = model.bert.embeddings.word_embeddings(batch[\"input_ids\"])\n",
    "        elif \"roberta-\" in model_name:\n",
    "            word_embedding = model.roberta.embeddings.word_embeddings(batch[\"input_ids\"])\n",
    "        \n",
    "        ## 1.2 初始化[抽样扰动delta_k]和[分布扰动mean_delta]\n",
    "        delta = perturbation.init_delta(word_embedding.size(), epsilon=adv_init_epsilon, init_type=adv_init_type)\n",
    "        delta.requires_grad = True\n",
    "        mean_delta = delta.detach().clone() # 初始化delta的分布均值mean_delta\n",
    "\n",
    "        ## 1.3 初始化模型输入inputs\n",
    "        if \"bert-\" in model_name:  # bert模型输入inputs: \"attention_mask\",\"labels\",\"token_type_ids\",「inputs_embeds」和「input_ids」参数二选一\n",
    "            inputs = {\"attention_mask\": batch[\"attention_mask\"],\"labels\": batch[\"labels\"], \"token_type_ids\": batch[\"token_type_ids\"]}\n",
    "        elif \"roberta-\" in model_name:  # roberta模型输入inputs: \"attention_mask\",\"labels\",「inputs_embeds」和「input_ids」参数二选一\n",
    "            inputs = {\"attention_mask\": batch[\"attention_mask\"], \"labels\": batch[\"labels\"]}\n",
    "\n",
    "        ## 1.4 备份模型参数\n",
    "        back_parameters = model.state_dict()\n",
    "        mean_theta = model.state_dict()\n",
    "\n",
    "        # 2.stochastic gradient langevin dynamics sampling\n",
    "        ## 2.1 sampling perturbation (delta)\n",
    "        for k in range(sampling_times_delta):\n",
    "            ### 构造带有扰动的输入\n",
    "            inputs[\"inputs_embeds\"] = delta + word_embedding.detach()\n",
    "            ### 前向传播\n",
    "            loss_adv = ls(model(**inputs).logits, model(**batch).logits)\n",
    "            ### 反向传播\n",
    "            loss_adv.backward()\n",
    "            ### SGLD采样\n",
    "            delta.data = SGLD(delta.data, - delta.grad, sampling_step_delta, sampling_noise_delta)\n",
    "            delta.grad = None\n",
    "            ### 更新扰动的分布均值\n",
    "            mean_delta.data = beta * mean_delta.data + (1 - beta) * delta.data\n",
    "\n",
    "        ## 2.2 sampling model parameters (theta)\n",
    "        for k in range(sampling_times_theta):\n",
    "            ### 清空模型参数的梯度\n",
    "            for p in model.parameters():\n",
    "                if p.grad!=None:\n",
    "                    p.grad.zero_()\n",
    "            ### 构造带有扰动的输入\n",
    "            if \"bert-\" in model_name:\n",
    "                word_embedding = model.bert.embeddings.word_embeddings(batch[\"input_ids\"])\n",
    "            elif \"roberta-\" in model_name:\n",
    "                word_embedding = model.roberta.embeddings.word_embeddings(batch[\"input_ids\"])\n",
    "            inputs[\"inputs_embeds\"] = mean_delta + word_embedding\n",
    "            ### 前向传播\n",
    "            loss_sum = model(**batch).loss + lambda_s * ls(model(**inputs).logits, model(**batch).logits)\n",
    "            ### 反向传播\n",
    "            loss_sum.backward()\n",
    "            ### SGLD采样\n",
    "            for p in model.parameters():\n",
    "                p.data = SGLD(p.data, p.grad, sampling_step_theta, sampling_noise_theta)\n",
    "            ### 更新模型参数的分布均值\n",
    "            new_model_param = model.state_dict()\n",
    "            for name in mean_theta:\n",
    "                mean_theta[name] = beta * mean_theta[name] + (1 - beta) * new_model_param[name]\n",
    "\n",
    "        # 3.update model parameters\n",
    "        for name in back_parameters:\n",
    "            back_parameters[name] = beta * back_parameters[name] + (1 - beta) * mean_theta[name]\n",
    "        model.load_state_dict(back_parameters) # 更新这次迭代的模型参数\n",
    "        # [end] MAT Training\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    print(\"\\rEvaling...\", end='', file=file)\n",
    "    model.eval()\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {key: batch[key].to(device) for key in batch}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        predictions = outputs.logits.argmax(dim=-1) if task_name != \"STS-B\" else outputs.logits.squeeze()\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    score = metric.compute()\n",
    "    eval_metric_list.append(score)\n",
    "    print(\"\\rMetric:\", score, file=file)\n",
    "    print(\"-\"*50, file=file)\n",
    "\n",
    "# Best score in eval\n",
    "score_list = []\n",
    "for m in eval_metric_list:\n",
    "    score_list.append(list(m.values())[0])\n",
    "print(\"*\"*19, \"Best Score\", \"*\"*19, file=file)\n",
    "print(\"EPOCH:\", score_list.index(max(score_list)), file=file)\n",
    "print(\"Metric:\", eval_metric_list[score_list.index(max(score_list))], file=file)\n",
    "print(\"*\"*50, file=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
