{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import preprocess\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置任务和模型\n",
    "task_name = \"CoLA\" # [\"CoLA\", \"SST-2\", \"MRPC\", \"STS-B\", \"QQP\", \"MNLI-m\", \"MNLI-mm\", \"QNLI\", \"RTE\", \"WNLI\"]\n",
    "model_name = \"models/bert-base-uncased\" # [bert-base-uncased, roberta-base]\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置超参数\n",
    "# 训练参数\n",
    "SEED = 42 # I don't understand why 42 , but everyone sets 42.\n",
    "BATCH_SIZE = 32\n",
    "EPOCH = 20\n",
    "LEARNING_RATE = 1e-5\n",
    "# 对抗训练参数\n",
    "Adv_step = 3\n",
    "Adv_epsilon = 1e-2\n",
    "Adv_max_norm = 2e-2  # 0表示不限制扰动大小\n",
    "lambda_s = 1\n",
    "mu = 1\n",
    "beta = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, dataloader, metric= preprocess.preprocess(task_name, model_name, BATCH_SIZE, SEED)\n",
    "train_dataloader,eval_dataloader,test_dataloader = dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置优化器\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "# lr_scheduler_name = [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"]\n",
    "lr_scheduler = get_scheduler(name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=EPOCH * len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "def ls(P, Q, task_type):\n",
    "    if(task_type == \"classification\"):\n",
    "        return F.kl_div(P.softmax(dim=-1).log(), Q.softmax(dim=-1), reduction='sum') + F.kl_div(Q.softmax(dim=-1).log(), P.softmax(dim=-1), reduction='sum')\n",
    "    elif(task_type == \"regression\"):\n",
    "        return MSELoss(P, Q, reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "print(\"*\"*20, \"Training\", \"*\"*20)  # 训练任务\n",
    "print(\"TASK:\", task_name)\n",
    "print(\"MODEL:\", model_name)\n",
    "print(\"DEVICE:\", device)\n",
    "print(\"-\"*16, \"General Training\", \"-\"*16) # 常规参数\n",
    "print(\"EPOCH_NUM:\", EPOCH)\n",
    "print(\"BATCH_SIZE:\", BATCH_SIZE)\n",
    "print(\"LEARNING_RATE:\", LEARNING_RATE)\n",
    "print(\"=\"*14, \"Adversarial Training\", \"=\"*14)  # 对抗训练参数\n",
    "print(\"Adversarial_Training_type:\",\"SMART\")\n",
    "print(\"Adversarial_step:\", Adv_step)\n",
    "print(\"Adversarial_epsilon:\", Adv_epsilon)\n",
    "print(\"Adversarial_max_norm:\", Adv_max_norm)\n",
    "print(\"lambda_s:\", lambda_s)\n",
    "print(\"mu:\", mu)\n",
    "print(\"beta:\", beta)\n",
    "print(\"*\"*50)\n",
    "model.to(device)\n",
    "old_parameters = model.parameters()\n",
    "progress_bar = tqdm(range(EPOCH * len(train_dataloader)))\n",
    "eval_metric_list = []\n",
    "k=0\n",
    "for i in range(EPOCH):\n",
    "    print(\"-\"*20, \"EPOCH:\", i, \"-\"*20)\n",
    "    print(\"Training...\", end='')\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        for t in batch:\n",
    "            batch[t] = batch[t].to(device)\n",
    "        \n",
    "        k=k+1\n",
    "        # [begin] Adversarial Training\n",
    "        # 1.compute L-loss \n",
    "        L_outputs=model(**batch)\n",
    "        L_loss = L_outputs.loss\n",
    "\n",
    "        # 2.compute R-loss\n",
    "        ## 2.1 init delta\n",
    "        if \"bert-base\" in model_name:\n",
    "            word_embedding_init = model.bert.embeddings.word_embeddings(batch[\"input_ids\"])\n",
    "        elif \"roberta-base\" in model_name:\n",
    "            word_embedding_init = model.roberta.embeddings.word_embeddings(batch[\"input_ids\"])\n",
    "        delta = torch.zeros_like(word_embedding_init)\n",
    "        adv_inputs = {\"attention_mask\" : batch[\"attention_mask\"], \"labels\" : batch[\"labels\"], \"token_type_ids\" : batch[\"token_type_ids\"]}\n",
    "        # 模型输入参数: \"attention_mask\",\"labels\",\"token_type_ids\",\"inputs_embeds\",「inputs_embeds」和「input_ids」参数二选一\n",
    "\n",
    "        ## 2.2 updata delta like PGD\n",
    "        for step in range(Adv_step):\n",
    "            delta.requires_grad = True\n",
    "            word_embedding = word_embedding_init.clone().detach()\n",
    "            adv_inputs[\"inputs_embeds\"] = delta + word_embedding # 使用添加扰动的「inputs_embeds」\n",
    "            outputs = model(**adv_inputs)\n",
    "            loss = outputs.loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward() \n",
    "\n",
    "            delta_grad = delta.grad.clone().detach() # delta的梯度\n",
    "            delta_grad_norm = torch.norm(delta_grad.view(delta_grad.size(0), -1), dim=1).view(-1, 1, 1) # .view：摊平  .norm：求L2范数  .view：重新设置shape\n",
    "            delta_grad_norm = torch.clamp(delta_grad_norm, min=1e-8)  # 设置最小值，避免下一步除数为0\n",
    "            delta = (delta + Adv_epsilon * delta_grad / delta_grad_norm).detach()  # 新的扰动 δ = δ + ε * g/||g||\n",
    "\n",
    "            if Adv_max_norm > 0:  # 限制扰动大小  Adv_max_norm = 0 则不限制\n",
    "                delta_norm = torch.norm(delta.view(delta.size(0), -1).float(), p=2, dim=1).detach()\n",
    "                exceed_mask = (delta_norm > Adv_max_norm).to(delta_norm)\n",
    "                if(sum(exceed_mask) != 0):  # 存在超出限制的扰动大小\n",
    "                    reweights = (Adv_max_norm / delta_norm * exceed_mask + (1-exceed_mask)).view(-1, 1, 1) # 缩减比例\n",
    "                    delta = (delta * reweights).detach()  # 按比例缩减到norm-ball内\n",
    "        ## 2.3 put the final delta into the model\n",
    "        adv_inputs[\"inputs_embeds\"] = delta + word_embedding_init.clone().detach()\n",
    "        R_outputs = model(**adv_inputs)\n",
    "        R_loss = ls(R_outputs.logits , L_outputs.logits , task_type = \"classification\" if task_name != \"STS-B\" else \"regression\")\n",
    "\n",
    "        # 3.compute D-loss\n",
    "        back_parameters = model.parameters()\n",
    "        for p,q in zip(model.parameters(),old_parameters):\n",
    "            p.data = q.data\n",
    "        D_outputs = model(**batch)\n",
    "        D_loss = ls(L_outputs.logits, D_outputs.logits, task_type = \"classification\" if task_name != \"STS-B\" else \"regression\")\n",
    "        for p,q in zip(model.parameters(),back_parameters):\n",
    "            p.data = q.data\n",
    "        old_parameters = model.parameters()\n",
    "        \n",
    "        # 4.optimize model parameters\n",
    "        loss = L_loss + lambda_s * R_loss + mu * D_loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        for p,q in zip(model.parameters(), old_parameters):\n",
    "            p.data =  (1-beta) * q.data + beta * p.data\n",
    "        progress_bar.update(1)\n",
    "        # [end] Adversarial Training\n",
    "\n",
    "    print(\"\\rEvaling...\", end='')\n",
    "    model.eval()\n",
    "    for batch in eval_dataloader:\n",
    "        for t in batch:\n",
    "            batch[t] = batch[t].to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        predictions = outputs.logits.argmax(dim=-1) if task_name != \"STS-B\" else outputs.logits.squeeze()\n",
    "        metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "    score = metric.compute()\n",
    "    eval_metric_list.append(score)\n",
    "    print(\"\\rMetric:\", score)\n",
    "    print(\"-\"*50)\n",
    "\n",
    "# Best score in eval\n",
    "score_list = []\n",
    "for m in eval_metric_list:\n",
    "    score_list.append(list(m.values())[0])\n",
    "print(\"*\"*19, \"Best Score\", \"*\"*19)\n",
    "print(\"EPOCH:\", score_list.index(max(score_list)))\n",
    "print(\"Metric:\", eval_metric_list[score_list.index(max(score_list))])\n",
    "print(\"*\"*50)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
